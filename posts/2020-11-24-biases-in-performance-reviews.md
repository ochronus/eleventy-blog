---
draft: false
author: Csaba Okrona
date: 2020-11-24
title: 7 harmful biases in performance reviews
permalink: /biases-in-performance-reviews/
excerpt: 'Unbiased performance reviews are key to a healthy organization and people’s growth. Learn about the typical biases in the process and how to counter them.'
tags:
    - performance review
    - management
image: /img/Typical-biases-in-performance-reviews-2048x1072.png
layout: layouts/post.njk
toc: true
---

We are all prone to biases. We cannot help but evaluate and assess people and situations through the lens of our own prejudices. When it comes to performance reviews this can have a huge unwanted impact as it influences compensation, promotion decisions, and even firing.

When you give a performance review for a colleague, you’re very directly impacting their career trajectory. Even so, if you’re their manager. Given the weight of this kind of influence, it’s our responsibility to make sure the reviews are as fair and objective as possible.

Not all is lost, though. Once you’re aware of the existence of these biases and the way they work, you can utilize certain strategies (and a good amount of self-awareness) to minimize their effect.

Below, I break down the most common performance review biases and share advice on how to deal with them both as the giver and the recipient of performance reviews.

<hr class="light-separator spacer-separator" />

## General advice for managers {data-toc-exclude}
One of the best ways to counter bias in reviews is to come up with a great review format that guides people and doesn’t magnify bias effects. Phrasing matters a lot. Setting the tone, being clear about the purpose and scope of the feedback form is key.

Another very important point is to understand that giving quality feedback takes time, a certain kind of focus, and is a considerable effort. Make sure you proactively prepare your engineers for the feedback season and plan for it – one thing that worked pretty well for me is to represent feedback tasks as cards on the team board and even set them as sprint goals. Nothing makes feedback quality deteriorate more than rushing and feeling that you don’t have enough time for it. You can organize feedback training, too, with our without your HR peers.

## General advice for everyone {data-toc-exclude}
On the flip side of the above – expect that giving feedback is not a trivial task. Take your time, make sure you have a quiet corner, don’t do it in one go, take notes, work on your wording, look at email/slack/pull request history too and treat your peers as customers of your feedback.

[My article on feedback](https://ochronus.online/thoughts-on-feedback/) might help in that.

<iframe loading=lazy src="https://ochronus.substack.com/embed" width="100%" height="320" style="border:none; background:#f5f5f5;" frameborder="0" scrolling="no"></iframe>

## Recency bias
Alice had a very strong year, she had great contributions to the projects her team was working on, achieved most of her goals, mastered a new language, and a framework. In the past month though, due to personal issues, she kept her involvement to the bare minimum. In his feedback to Alice, Bob highlights that he expects more from her and that she feels distant from the team. Bob completely fails to call out the amazing job Alice did earlier and the growth she had had.

Recency bias is when recent events weigh much more heavily in your performance review than older, possibly even more significant events. This is partly due to how our memory works and is a completely natural thing, yet its impact can be really bad and can bias your review in either direction depending on what people remember about you recently.

### How to deal with recency bias {data-toc-exclude}
The best way is to collect feedback more frequently – for example, do a 360 each quarter even if you only have the performance review once a year. Project-level retrospectives can be helpful as well. Some managers keep ‘files’ on their engineers to counter this bias, but honestly, that can easily backfire – it can feel like a shady practice to their teams. Prefer transparent and open frequent feedback instead.

Some people find it useful to keep a personal achievement log, which helps with their self-assessment or calling out things missing from their feedback. If you feel there are important bits missing from the feedback you’ve been given, call those out. If your manager doesn’t encourage more frequent feedback you can still ask for informal ones from your peers at any time.

## Similarity bias and Affinity bias
Alice and Bob graduated from the same university and are both huge Star Trek fans – they talk about it all the time, they get along really well and connect outside work, too. Bob’s feedback to Alice is always overly positive and he’s prone to overlooking seemingly obvious gaps in her performance.

We subconsciously tend to rate people similar to us higher. Similarity can mean many things – personality, looks, way of thinking, etc. Affinity bias occurs when we work with someone we feel we have an affinity with; maybe we attended the same college, we grew up in the same town, or they remind us of someone we know and like.

### How to deal with similarity and affinity bias {data-toc-exclude}
A clear, and transparent performance evaluation system helps a lot here. Such a system is clear and well-understood level definitions, which can guide your focus while thinking about others’ performance. That said, levels are usually not public information in companies, so this won’t help too much with peer review.

Getting feedback from multiple peers can help mitigate the effect of this bias.

## Halo effect and horn effect
Alice is really great at debugging and fixing notoriously tricky bugs others usually struggle with. Because of this, she saved the day multiple times. That said, she barely meets her level’s expectations if we look at the wider spectrum. Alice gets really positive feedback from her team highlighting how grateful they are for her being the ‘bug hunter’ and omitting any gaps she might have elsewhere.

Bob meets his level’s expectations in general and is great to work with. That said, he has the tendency to be impatient and cut discussions short because of that, which really hurts his ability to effectively work with others in these situations. Bob gets negative feedback highlighting that he should really work on his temper and communication – not mentioning any of the amazing work he’s done otherwise.

In the halo effect, a single positive event or attribute lifts your review up, and in the horn effect similarly a single negatively perceived action or trait ‘poisons’ your whole review.

This gets even worse if your manager is biased. A classic example of the manager having a halo bias is when they see one of their engineers as the “hero” or the “10x engineer”, being blind about any gaps they might have (btw. check out my older post about hero engineers). An example of a manager having the horn bias is when they stigmatize an engineer as e.g. “unreliable” or “not smart enough” based on a one-off event.

### How to counter the halo or horn effects {data-toc-exclude}
You might ask “why would I want to counter the halo effect if it results in positive reviews about me?”. True, it might momentarily be even helpful for you, but not having a clear picture of your gaps ultimately does more damage than good to your career. It hinders your potential to grow and if you change teams, managers, or companies you might be suddenly underperforming and you won’t necessarily understand what happened.

To counter the effect of these biases you first need to understand what the main positive or negative thing is in your feedback and have a heart-to-heart about it with yourself. Again, a proper level definition system helps a lot. If you feel that people are generalizing a one-off negative event, ask them to provide more examples of that behavior. You can actually call out that you feel stigmatized by that single event or trait. If you can, highlight counterexamples.

Sometimes phrasing of rating scale points helps mitigate these biases, e.g. if you call the two extremes of the scales “consistently underperforming” and “top performer”.


## Idiosyncratic rater bias
Bob is an engineering manager leading a mobile developer team. Bob has deep experience in project management but almost none in mobile development. Bob seems to consistently rate the development skills of his engineers much higher than they really are, while he rates the project management performance of the lead developer lower than it is.

Idiosyncratic rater bias happens when people evaluate skills they’re not good at, higher. Sometimes they rate others lower in things they’re great at. This is rooted in lower standards we have for things we don’t have deep knowledge about and higher standards for things we’re experienced at. In other words, our feedback reflects more on our own skills than the person’s we’re reviewing.

### How to counter the idiosyncratic rater bias {data-toc-exclude}
To overcome this bias as a manager, try rephrasing your performance evaluation questions for yourself from a different perspective, e.g.:

If this engineer wanted to resign I would try to retain them.

I would want this engineer on my team at any time.

I would hire this engineer again at any time.

Research shows that people are much more accurate when rating their own intentions compared to rating other people.

Also, having a diverse set of feedback from peers can mitigate this (there’s a low probability for every reviewer to be biased the same way).

## Centrality bias
Alice is the manager of a team. She hands in her annual performance evaluations, and you notice that almost everyone on her team scored near the middle of the scale. Now you wonder if that’s actually a realistic image or not.

Many managers don’t like being extreme and tend to be moderate in their reviews. When everyone is receiving a rating of 3 out of 5 across the board, there’s no distinguishing the low-performing and high-performing employees. This will result in unfair reviews and people being pissed by lack of recognition and that nothing happens to low performers.

### How to counter the centrality bias {data-toc-exclude}
Well, an easy hack is to remove the middle of the rating scale, the ‘neutral’ option, e.g. have a scale of 4 instead of 5 to force managers to decide. If you received one of the ‘meh’ reviews, have a heart-to-heart with your manager and highlight where you disagree. For example, if you feel you’ve been doing better in a certain area ask for explicit examples of how you could do better and cross-check it with your data points. Rating on multiple skills and axes can help, too, compared to a single, unified rating.


## Contrast bias
Alice is really good at project management. Bob is also great at it, but slightly less so than Alice. Their manager rates Bob low on project management because he cannot help comparing him to Alice. In reality, both are exceeding expectations on their level with regards to their project management skills.

Contrast bias occurs when the manager compares an employee’s performance to other employees instead of the company standard. When employees are ranked in comparison, someone must end up at the bottom, even if they are exceeding the company standard. The problem isn’t the employee – it’s the goal or standard that has been set.

### How to counter the contrast bias {data-toc-exclude}
The review should have multiple axes and expectations should be clearly laid out in e.g. a leveling system. This doesn’t automatically protect against the contrast bias but it makes it harder to happen. What I’ve found useful as a manager is to separate each review I write with extra time and do something different in between – e.g. write the review for each engineer on separate days. If you feel you’ve been the victim of the contrast bias, always move to refer to the expectation definitions (e.g. leveling system). Point out to your manager how you think you’re meeting/exceeding specific expectations.


## Confirmation bias
Alice, Bob’s manager thinks Bob is a great person to work with. They understand each other very well, communication has always been a real pleasure. Alice reads Bob’s peer feedback from Carol, which says Bob’s communication is not so great. Alice dismisses this information.

Alice also manages Carol, who she believes to be a bit too junior. Carol gets overly positive feedback, except for one which mentions an occasion when her pull request was just not up to a certain standard. Alice immediately confirms her belief that Carol has so much to work on.

Confirmation bias is the tendency to search for or interpret new information in a way that confirms our preexisting beliefs. We intrinsically find it easy to believe people who align with us on specific facts, beliefs, or stances. On the other hand, we’re likely to be skeptical of people who don’t agree with us. It can skew the interpretation of valuable performance data.

### How to counter the confirmation bias {data-toc-exclude}
Turn this around! Look for information that goes against what you believe. Try to find data for it! If you read something that confirms what you already think, immediately check how many times others mention its opposite.

<hr class="light-separator spacer-separator" />

Fighting these biases is not trivial, but as professionals and especially as people managers we have to work hard to be fair, objective, and especially helpful in performance reviews. Knowing your biases will certainly help, but it’s only the first step – I invite you to think back to your latest review cycle and see if you’ve been biased.

There are a lot more biases than the ones listed above – [Wikipedia](https://en.wikipedia.org/wiki/List_of_cognitive_biases) has an amazing collection if you’re interested.

Gergely Orosz has a really spot-on article about this topic as well, I highly recommend reading it: [Common Performance Review Biases: How to Spot and Counter Them](https://blog.pragmaticengineer.com/performance-review-biases/)

